{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08a40d1f",
      "metadata": {
        "id": "08a40d1f"
      },
      "source": [
        "### downloading data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PZ1wkPfKmEUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc0a1f2-496d-4e68-a1ff-e0a6184cc148"
      },
      "id": "PZ1wkPfKmEUe",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to the downloaded zip file on Google Drive\n",
        "zip_file_path = \"/content/drive/MyDrive/celebS.zip\"\n",
        "\n",
        "# Create the directory to extract to\n",
        "extract_dir = Path(\"images\")\n",
        "extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Extract the contents\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Extracted files to: {extract_dir}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {zip_file_path} was not found.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file {zip_file_path} is not a valid zip file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjXv2S0OmLXI",
        "outputId": "f248d9d9-34f2-4369-edb7-d965c97bae78"
      },
      "id": "VjXv2S0OmLXI",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files to: images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dir = Path(\"images/img_align_celeba\") # Assuming the images are in a subdirectory after extraction\n",
        "train_dir = Path(\"images/train\")\n",
        "valid_dir = Path(\"images/valid\")\n",
        "\n",
        "# Create destination directories\n",
        "train_dir.mkdir(parents=True, exist_ok=True)\n",
        "valid_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Get list of image files\n",
        "image_files = list(source_dir.glob(\"*.jpg\"))\n",
        "\n",
        "# Define split ratio (e.g., 80% train, 20% valid)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_files) * split_ratio)\n",
        "\n",
        "# Split files\n",
        "train_files = image_files[:split_index]\n",
        "valid_files = image_files[split_index:]\n",
        "\n",
        "# Move files to respective directories\n",
        "print(\"Moving training images...\")\n",
        "for file in train_files:\n",
        "    shutil.move(str(file), str(train_dir / file.name))\n",
        "\n",
        "print(\"Moving validation images...\")\n",
        "for file in valid_files:\n",
        "    shutil.move(str(file), str(valid_dir / file.name))\n",
        "\n",
        "print(\"Image splitting complete.\")"
      ],
      "metadata": {
        "id": "THeic5aZmOmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0741881a-18bc-49ac-d57e-f75c1d74eaee"
      },
      "id": "THeic5aZmOmR",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving training images...\n",
            "Moving validation images...\n",
            "Image splitting complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7891dd5",
      "metadata": {
        "id": "f7891dd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769d03f6-8888-4d67-8daf-e1f2abb8a63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving training images...\n",
            "Moving validation images...\n",
            "Image splitting complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dir = Path(\"images/img_align_celeba\") # Assuming the images are in a subdirectory after extraction\n",
        "train_dir = Path(\"images/train\")\n",
        "valid_dir = Path(\"images/valid\")\n",
        "\n",
        "# Create destination directories\n",
        "train_dir.mkdir(parents=True, exist_ok=True)\n",
        "valid_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Get list of image files\n",
        "image_files = list(source_dir.glob(\"*.jpg\"))\n",
        "\n",
        "# Define split ratio (e.g., 80% train, 20% valid)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_files) * split_ratio)\n",
        "\n",
        "# Split files\n",
        "train_files = image_files[:split_index]\n",
        "valid_files = image_files[split_index:]\n",
        "\n",
        "# Move files to respective directories\n",
        "print(\"Moving training images...\")\n",
        "for file in train_files:\n",
        "    shutil.move(str(file), str(train_dir / file.name))\n",
        "\n",
        "print(\"Moving validation images...\")\n",
        "for file in valid_files:\n",
        "    shutil.move(str(file), str(valid_dir / file.name))\n",
        "\n",
        "print(\"Image splitting complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install piq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_ZchCjXm1kQ",
        "outputId": "7a96bc75-b326-4e21-a890-9fd757e8dba1"
      },
      "id": "R_ZchCjXm1kQ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting piq\n",
            "  Downloading piq-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from piq) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->piq) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->piq) (2.8.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->piq) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision>=0.10.0->piq) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision>=0.10.0->piq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision>=0.10.0->piq) (3.0.2)\n",
            "Downloading piq-0.8.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: piq\n",
            "Successfully installed piq-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0c7adf72",
      "metadata": {
        "id": "0c7adf72"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms as tf\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from piq import ssim\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "12a3065e",
      "metadata": {
        "id": "12a3065e"
      },
      "outputs": [],
      "source": [
        "class UFaceDataset(Dataset):\n",
        "    def __init__(self, path, transform):\n",
        "        super().__init__()\n",
        "        self.paths = [p for p in Path(path).rglob(\"*.jpg\")]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            img = Image.open(self.paths[index])\n",
        "            main = self.transform(img)\n",
        "\n",
        "            kernel_size = random.choice([3, 5, 7])\n",
        "            sigma = random.uniform(0.1, 2.0)\n",
        "            blured = F.gaussian_blur(main, kernel_size=kernel_size, sigma=sigma)\n",
        "\n",
        "            return {\n",
        "                'main': main,\n",
        "                'blured': blured\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error while opening image at index {index}, path: {self.paths[index]}\")\n",
        "            raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "735d0e9d",
      "metadata": {
        "id": "735d0e9d"
      },
      "outputs": [],
      "source": [
        "class FaceUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # feature size / 2\n",
        "        # channel 3 -> 64\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),  # /2\n",
        "        )\n",
        "\n",
        "        # feature size * 2\n",
        "        # channel 64 -> 3\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # after concatenating output of up 3 with the input image\n",
        "        self.conv1 = nn.Conv2d(6, 3, kernel_size=3, padding=1, stride=1)  # 3 channels\n",
        "\n",
        "        # ===============================================================================================\n",
        "\n",
        "        # feature size / 2\n",
        "        # channel 64 -> 128\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),  # [128]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),  # /2\n",
        "        )\n",
        "\n",
        "        # feature size * 2\n",
        "        # channel 128 -> 64\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # after concatenating output of up 2 with output of down 1\n",
        "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, padding=1, stride=1)  # 64 channels\n",
        "\n",
        "        # ===============================================================================================\n",
        "\n",
        "        # feature size / 2\n",
        "        # channel 128 -> 256\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),  # [256]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),  # /2\n",
        "        )\n",
        "\n",
        "        # feature size * 2\n",
        "        # channel 256 -> 128\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # *2 , [64]\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # after concatenating output of up 1 with output of down 2\n",
        "        self.conv3 = nn.Conv2d(256, 128,  kernel_size=3, padding=1, stride=1)  # 128 channels\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity0 = x  # c = 3\n",
        "\n",
        "        # ==--==--==--==-\n",
        "        x = self.down1(x)  # c = 64\n",
        "        identity1 = x  # c = 64\n",
        "\n",
        "        # ==--==--==--==-\n",
        "        x = self.down2(x)  # c = 128\n",
        "        identity2 = x  # c = 128\n",
        "\n",
        "        # ==--==--==--==-\n",
        "        x = self.down3(x)  # c = 256\n",
        "\n",
        "        # ==--==--==--==-\n",
        "        x = self.up3(x)  # c = 128\n",
        "        x = torch.cat([x, identity2], dim=1)  # c = 256\n",
        "        x = self.conv3(x)  # c = 128  |  recover the channels again\n",
        "\n",
        "        # ==--==--==--==-\n",
        "        x = self.up2(x)  # c = 64\n",
        "        x = torch.cat([x, identity1], dim=1)  # c = 128\n",
        "        x = self.conv2(x)  # c = 64  |  recover the channels again\n",
        "\n",
        "        # ==--==--==--==-\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, identity0], dim=1)  # c = 6\n",
        "        x = self.sigmoid(self.conv1(x))  # c = 3  |  recover the channels again\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "24feaf37",
      "metadata": {
        "id": "24feaf37"
      },
      "outputs": [],
      "source": [
        "class FaceUNetLoss(nn.Module):\n",
        "    def __init__(self, lambda_ssim: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.lambda_ssim = lambda_ssim\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        mse_part = self.mse_loss(pred, target)\n",
        "        ssim_part = 1 - ssim(pred, target)\n",
        "        return mse_part + self.lambda_ssim * ssim_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c70e1c71",
      "metadata": {
        "id": "c70e1c71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19719d80-d729-426b-bed9-225c2979b877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting operation on device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"starting operation on device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9d3c430c",
      "metadata": {
        "id": "9d3c430c"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b4028faa",
      "metadata": {
        "id": "b4028faa"
      },
      "outputs": [],
      "source": [
        "# paths\n",
        "train_path = \"images/train\"\n",
        "valid_path = \"images/valid\"\n",
        "\n",
        "# transforms\n",
        "transform = tf.Compose([\n",
        "    tf.Resize([176, 216]),\n",
        "    tf.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b63f5a85",
      "metadata": {
        "id": "b63f5a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e10a2a-7fba-4e41-c558-d4a7a33a9397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 162079 training images\n",
            "found 40520 validation images\n"
          ]
        }
      ],
      "source": [
        "train_dataset = UFaceDataset(train_path, transform)\n",
        "valid_dataset = UFaceDataset(valid_path, transform)\n",
        "\n",
        "print(f\"found {len(train_dataset)} training images\")\n",
        "print(f\"found {len(valid_dataset)} validation images\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "59dbc44f",
      "metadata": {
        "id": "59dbc44f"
      },
      "outputs": [],
      "source": [
        "model = FaceUNet().to(DEVICE)\n",
        "criterion = FaceUNetLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
        "epochs = 15\n",
        "\n",
        "# early stopping\n",
        "patience_number = 3\n",
        "delta = 0.001\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 0\n",
        "\n",
        "# outputs\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "os.makedirs(\"outputs/checkpoints\", exist_ok=True)\n",
        "train_losses = []\n",
        "val_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f487cf2",
      "metadata": {
        "id": "1f487cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ad152f-fb84-41b6-802c-b41d4191eca0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Start training with 15 epochs ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training  : 100%|██████████| 2533/2533 [28:15<00:00,  1.49it/s]\n",
            "validating: 100%|██████████| 634/634 [04:27<00:00,  2.37it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 0.0218 | Val Loss: 0.0122✅ Validation improved. Saving model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training  : 100%|██████████| 2533/2533 [27:22<00:00,  1.54it/s]\n",
            "validating: 100%|██████████| 634/634 [04:23<00:00,  2.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train Loss: 0.0106 | Val Loss: 0.0095✅ Validation improved. Saving model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training  : 100%|██████████| 2533/2533 [27:32<00:00,  1.53it/s]\n",
            "validating: 100%|██████████| 634/634 [04:26<00:00,  2.38it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train Loss: 0.0090 | Val Loss: 0.0085✅ Validation improved. Saving model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training  : 100%|██████████| 2533/2533 [27:42<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 634/634 [04:24<00:00,  2.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train Loss: 0.0083 | Val Loss: 0.0085⚠️ No improvement. Early stop counter: 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training  : 100%|██████████| 2533/2533 [27:44<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 634/634 [04:25<00:00,  2.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train Loss: 0.0079 | Val Loss: 0.0076⚠️ No improvement. Early stop counter: 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training  : 100%|██████████| 2533/2533 [27:41<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 634/634 [04:26<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6] Train Loss: 0.0076 | Val Loss: 0.0074✅ Validation improved. Saving model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training  :  68%|██████▊   | 1712/2533 [18:48<08:58,  1.52it/s]"
          ]
        }
      ],
      "source": [
        "print(f\"🚀 Start training with {epochs} epochs ...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm.tqdm(train_loader, total=len(train_loader), desc=\"training  \"):\n",
        "        blrd_img, main_img = batch[\"blured\"].to(DEVICE), batch[\"main\"].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(blrd_img)\n",
        "        loss = criterion(output, main_img)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(valid_loader, total=len(valid_loader), desc=\"validating\"):\n",
        "            blrd_img, main_img = batch[\"blured\"].to(DEVICE), batch[\"main\"].to(DEVICE)\n",
        "\n",
        "            output = model(blrd_img)\n",
        "            loss = criterion(output, main_img)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = valid_loss / len(valid_loader)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\", end=\"\")\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # early stopping\n",
        "    if avg_val_loss < best_val_loss - delta:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience = 0\n",
        "        torch.save(model.state_dict(), \"outputs/checkpoints/superres_best.pth\")\n",
        "        print(f\"✅ Validation improved. Saving model.\")\n",
        "    else:\n",
        "        patience += 1\n",
        "        print(f\"⚠️ No improvement. Early stop counter: {patience}/{patience_number}\")\n",
        "        if patience >= patience_number:\n",
        "            print(\"⛔ Early stopping triggered!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a92fc9",
      "metadata": {
        "id": "55a92fc9"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_losses, val_losses)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}